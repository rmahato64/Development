{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is LikeliHood function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many probability distributions have unknown parameters; We estimate these unknowns using sample data. The Likelihood function gives us an idea of how well the data summarizes these parameters.\n",
    "\n",
    "The “parameters” here aren’t population parameters— they are the parameters for a particular probability distribution function (PDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood function is the probability of the observed data as a function of unknown parameter.\n",
    "Mathematically, \n",
    "\\begin{equation}\n",
    "L(\\theta, \\boldsymbol{x})=\\prod_{i=1}^{n} f_{i}\\left(x_{i}\\right)=\\prod_{i=1}^{n} \\theta_{i}^{x_{i}}\\left(1-\\theta_{i}\\right)^{1-x_{i}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where ,\n",
    "\\begin{equation}\n",
    "L(\\theta) = likelihood-function\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "x = observed-data \n",
    "\\end{equation} \n",
    "\n",
    "\\begin{equation}\n",
    "\\theta= unknown-parameter\n",
    "\\end{equation}\n",
    "\n",
    "    1.Unlike pdf, likelihoods aren’t normalized. The area under curve does not have to be 1.\n",
    "    2.We also use likelihoods to generate estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define Likelihood function as f(parameter, data) is the PDF and L(data, paramater) is the likelihood function where data is the vector of observations\n",
    "\n",
    "We can determine Likelihood function for any kind of distribution.\n",
    "eg: for Normal, Bernouli, Binomial, Poisson etc.\n",
    "\n",
    "### We have 4 steps in determining the Likelihood function.\n",
    "\n",
    "1) Find the joint distribution of indedpendent variables. It is nothing but the product of independent PDF's.\n",
    "\n",
    "2) Find the logarithm of the obtained Likelihood function from step 1, because this will make the higher values to get comptessed.\n",
    "\n",
    "3) Our aim in this step is to find the MLE(Maximum Likelihood Estimator).\n",
    "MLE :\n",
    " Maximum Likelihood Estimator is nothing but the point where:\n",
    " \n",
    "  Maximum likelihood estimation (MLE) is a method of estimating the parameters of a distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate.\n",
    "  \n",
    "4 ) So to find the MLE, we need to take the derivative of the obtained function and make it equal to zero.\n",
    "We are doing this because the point where derivative is 0 is called the Maximum point, in order to find that point we are doing this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formula for LikeliHood Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the joint probability density function of your sample X = (X1,…X2) is f(x| θ), where θ is a parameter. X = x is an observed sample point. Then the function of θ defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L(θ |x) = f(x |θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://mathworld.wolfram.com/MaximumLikelihood.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##MLE for LINEAR REGRESSION & Classification. Introduction to Regularization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is linear regression related to Pytorch and gradient descent? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/dsnet/linear-regression-with-pytorch-3dde91d60b50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is an algorithm which is used to estimate the dependent variable based on independent variables.\n",
    "\n",
    "In a linear regression model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eg : y = mx+c is the regression line, then x is the indendent variable, y is the dependent variable and m and c are weights and bias.\n",
    "    In a linear regression model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias:\n",
    "    \n",
    "                        y_1  = w11 * x11 + w12 * x12 + w13 * x13 + b1\n",
    "                        y_2 = w21 * x21 + w22 * x22 + w23 * x23 + b2 \n",
    "    \n",
    "The learning part of linear regression is to figure out a set of weights w11, w12,... w23, b1 & b2 by looking at the training data, to make accurate predictions for new data (i.e. to predict the yields for apples and oranges in a new region using the average temperature, rainfall and humidity). This is done by adjusting the weights slightly many times to make better predictions, using an optimization technique called gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function in Linear Regression & MSE:\n",
    "\n",
    "Before we improve our model, we need a way to evaluate how well our model is performing. We can compare the model’s predictions with the actual targets, using the following method:\n",
    "\n",
    "1) Calculate the difference between the two matrices (preds and targets).\n",
    "\n",
    "2 ) Square all elements of the difference matrix to remove negative values.\n",
    "\n",
    "3) Calculate the average of the elements in the resulting matrix.\n",
    "The result is a single number, known as the mean squared error (MSE).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is a quadratic function of our weights and biases, and our objective is to find the set of weights where the loss is the lowest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjust weights and biases using gradient descent\n",
    "We’ll reduce the loss and improve our model using the gradient descent optimization algorithm, which has the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Generate predictions\n",
    "\n",
    "2.Calculate the loss\n",
    "\n",
    "3.Compute gradients w.r.t the weights and biases\n",
    "\n",
    "4.Adjust the weights by subtracting a small quantity proportional to the gradient\n",
    "\n",
    "5.Reset the gradients to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch has some predefined libraries and functions which is used to calculate the gradient descent for loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we cannot use MSE or MAE or other regression cost functions to calculate the loss for classification models.\n",
    "It has different models for classfication and neural network problems.\n",
    "Eg of classification : Cross entropy loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$M S E=\\frac{1}{N} \\sum_{i=1}^{n}\\left(y_{i}-\\left(m x_{i}+b\\right)\\right)^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation btwn MSE and Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.jessicayung.com/mse-as-maximum-likelihood/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally MSE is used to calculate the error or loss function. By using the likelihood function we can determine the maximum point at which derivative is 0 which determines the MSE value is less, which inturn is use to minimize the\n",
    "mean sqaured error of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is related in a way which is is used to find or optimized value for Mean sqaured error with respect to weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##  Can gradient descent be used to find the parameters for linear regression? What about linear classification? Why?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent is used in Linear regression to find the optimized parameters. and is used to update the values of weights using gradient Descent algorithm.\n",
    "\n",
    "In linear classification it is hard to find the paramaters and optimize the cost funcrion using Gradient Descent becausethe boundaries are not clear until it reaches the data point as there is no local information on the point in which it is moving.\n",
    "Hence we have algorithm to estimate parametes silimar to this as regression i.e Perceptron Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  What are normal equations? Is it the same as least squares? Explain. You may refer to this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal Equation is an analytical approach to Linear Regression with a Least Square Cost Function. We can directly find out the value of θ without using Gradient Descent. Following this approach is an effective and a time-saving option when are working with a dataset with small features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "J(\\mathbf{w})=\\|\\mathbf{e}\\|^{2}=\\|\\mathbf{y}-X \\mathbf{w}\\|^{2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "w=\\left(X^{T} X\\right)^{-1} X^{T} y\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Have to write the normal equation in latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is feature scaling needed for linear regression when using gradient descent?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.quora.com/Do-I-need-to-do-feature-scaling-for-simple-linear-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually Feature scaling in model depends on the optimization algorithm that we are using in the model. \n",
    "\n",
    "If we are using any regularization (L1/L2 for example) because in this case scaling can affect solution.\n",
    "\n",
    "But if we are using cllosed form solution (solving system of linear equations by some exact solver) you do not need scaling.\n",
    "\n",
    "But if we are using some iterative algorithm (gradient descent for example) scaling can make convergence faster.\n",
    "\n",
    "Example :\n",
    "If we are performing a OLS regression . There are two ways to solve for the optimal solution. we can solve it via the analytical solution or via an iterative algorithm such as gradient descent.\n",
    "\n",
    "If we are using the analytical solution, feature scaling wont be of much use. In fact, we may want to refrain from feature scaling so that the model is more comprehensive. However, if you are using the gradient descent algorithm, feature scaling will help the solution converge in a shorter period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out the MLE approach for logistic regression. How is this related to the binary cross-entropy? answer 5 and 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s consider the logistic model (a binary classifier) to describe log-odds using a linear model:\n",
    "\n",
    "$$\n",
    "\\ln \\frac{p}{1-p}=\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{m} x_{m}\n",
    "$$<b>\n",
    "\n",
    "The probability of observing outcome y=1 under this model is given by the following function (sigmoid):<b>\n",
    "$$\n",
    "p \\equiv p(y=1 | \\mathbf{B}, \\mathbf{X})=\\frac{1}{1+e^{-\\left(\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{m} x_{m}\\right)}}\n",
    "$$<b>\n",
    "\n",
    "With 0 and 1 being the only possible outcomes, the probability of observing outcome y=0 is simply (1-p):\n",
    "\n",
    "$$\n",
    "p(y=o | \\mathbf{B}, \\mathbf{X})=p^{o}(1-p)^{1-o}\n",
    "$$\n",
    "\n",
    "The likelihood function is given by the product of all individual probabilities:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}=\\prod_{i=1}^{n} p\\left(y=y_{i} | \\mathbf{B}_{i}, \\mathbf{X}_{i}\\right)=\\prod_{i=1}^{n} p_{i}^{y_{i}}\\left(1-p_{i}\\right)^{1-y_{i}}\n",
    "$$\n",
    "\n",
    "It’s easier to maximize the log-likelihood:\n",
    "\n",
    "$$\n",
    "\\ln \\mathcal{L}=\\sum_{i=1}^{n}\\left(y_{i} \\ln p_{i}+\\left(1-y_{i}\\right) \\ln \\left(1-p_{i}\\right)\\right)\n",
    "$$\n",
    "Thus  maximum liklihood estimation yields a familiar loss function (cross-entropy in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 and L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is used to reduce the overfitting of data or model. There are he model will have a low accuracy if it is overfitting. This happens because your model is trying too hard to capture the noise in your training dataset.\n",
    "One of the ways of avoiding overfitting is using cross validation, that helps in estimating the error over test set, and in deciding what parameters work best for your model.\n",
    "\n",
    "This is a form of regression, that constrains/ regularizes or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting.\n",
    "\n",
    "                     Y ≈ β0 + β1X1 + β2X2 + …+ βpXp\n",
    "\n",
    "A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression.\n",
    "\n",
    "\n",
    "Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function. Here the highlighted part represents L2 regularization element.\n",
    "\n",
    "Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function. Here the highlighted part represents L2 regularization element.\n",
    "\n",
    "### Ridge Regression:\n",
    "    \n",
    "\n",
    "    \n",
    "Here, if lambda is zero then you can imagine we get back OLS. However, if lambda is very large then it will add too much weight and it will lead to under-fitting. Having said that it’s important how lambda is chosen. This technique works very well to avoid over-fitting issue.\n",
    "\n",
    "### Lasso Regression:\n",
    "    \n",
    "(Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude” of coefficient as penalty term to the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use Normal and Gradient Descent :\n",
    "    \n",
    "When we have more features then we need to use Gradient descent over millions and when we have small data set then we can use normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{array}{l}{J(\\theta)=\\frac{1}{2 m}\\left[\\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}+\\lambda \\sum_{j=1}^{n} \\theta_{j}^{2}\\right]} \\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "Following is the normal equation,\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta=\\left(X^{T} X\\right)^{-1} X^{T} y\n",
    "\\end{equation}\n",
    "\n",
    "To use regularization we add a term (+ λ I(n+1*n+1)) to the equation. Then the regularized normal equation is \n",
    "\\begin{equation}\n",
    "\\theta=\\left(X^{T} X+\\lambda\\left[\\begin{array}{ccccc}{0} \\\\ {} & {1} \\\\ {} & {} & {1} \\\\ {} & {} & {} & {\\ddots} \\\\ {} & {} & {} & {} & {1}\\end{array}\\right]\\right)^{-1} X^{T} y\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
