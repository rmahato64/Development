{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function is Symmetric:\n",
    "If we prove that derivative of x is similar to -x then we can prove that function is symmetrical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "            \\label{eq:sigmoid_function_derivative}\n",
    "            \\frac{d\\sigma(x)}{dx} = \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "            \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that it's very easy to compute the derivative of the sigmoid function if you've already calculated the sigmoid function itself. E.g. when backpropagating errors in a neural network through a layer of nodes with a sigmoid activation function, \n",
    "\\sigma(x) has already been computed during the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to differentiate the sigmoid function as shown in equation \n",
    "we'll first derive:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\t\t\t\\label{eq:sigmoid_function_derivative_sigma_x_times_sigma_minus_x}\n",
    "            \\frac{d\\sigma(x)}{dx} = \\sigma(x) \\cdot \\sigma(-x)\n",
    "\t\t\t\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then equation follows directly from the above fact combined with equation which tells us that \\sigma(-x) = 1 - \\sigma(x) So here goes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "            \\frac{d\\sigma(x)}{dx} &= \\frac{d}{dx}(\\frac{1}{1 + e^{-x}}) \\notag \\\\\n",
    "                      &= \\frac{d}{dx}(1 + e^{-x})^{-1} \\notag \\\\\n",
    "                      &=  -(1 + e^{-x})^{-2} \\cdot (-e^{-x}) \\notag \\\\\n",
    "                      &=  \\frac{e^{-x}}{(1 + e^{-x})^{2}} \\notag \\\\\n",
    "                      &= \\frac{1}{1 + e^{-x}} \\cdot \\frac{e^{-x}}{1 + e^{-x}} \\notag \\\\\n",
    "                      &= \\sigma(x) \\cdot \\sigma(-x) \\notag \\\\ \n",
    "            \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the last equality follows directly from equation \n",
    "(1)\n",
    " (please refer to the margin note for \n",
    "(1)\n",
    ", for the alternate form of the sigmoid function).\n",
    "\n",
    "As should be evident from the graph of the derivative of the sigmoid function it's symmetric across the vertical axis, that is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "            \\label{eq:sigmoid_function_derivative_symmetri}\n",
    "            \\frac{d\\sigma(x)}{dx} = \\frac{d\\sigma(-x)}{dx}\n",
    "            \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which proves that sigmoid function is symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of Sigmoid Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align} \n",
    "\\frac{d}{dx}\\sigma(x) &= (\\frac{1}{1+e^{-x}})' \\\\\n",
    "&= -\\frac{1}{(1+e^{-x})^{2}} \\cdot (1) \\cdot -e^{-x}  \\\\\n",
    "&= \\frac{e^{-x}}{(1+e^{-x})^{2}}, \\\\\n",
    "\\because \\sigma(x) &= \\frac{1}{1+e^{-x}}, \\\\\n",
    "e^{-x} &= \\frac{1 - \\sigma(x)}{\\sigma(x)}, \\\\\n",
    "1+e^{-x} &= \\frac{1}{\\sigma(x)}; \\\\\n",
    "\\therefore \\frac{d}{dx}\\sigma(x) &= \\frac{\\frac{1 - \\sigma(x)}{\\sigma(x)}}{(\\frac{1}{\\sigma(x)})^{2}} \\\\\n",
    "&= (1 - \\sigma(x)) \\cdot \\sigma(x)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 and L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T08:53:53.746144Z",
     "start_time": "2019-08-06T08:53:53.729441Z"
    }
   },
   "source": [
    "This is used to reduce the overfitting of data or model. There are he model will have a low accuracy if it is overfitting. This happens because your model is trying too hard to capture the noise in your training dataset.\n",
    "One of the ways of avoiding overfitting is using cross validation, that helps in estimating the error over test set, and in deciding what parameters work best for your model.\n",
    "\n",
    "This is a form of regression, that constrains/ regularizes or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting.\n",
    "\n",
    "                     Y ≈ β0 + β1X1 + β2X2 + …+ βpXp\n",
    "\n",
    "A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression.\n",
    "\n",
    "\n",
    "Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function. Here the highlighted part represents L2 regularization element.\n",
    "\n",
    "Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function. Here the highlighted part represents L2 regularization element.\n",
    "\n",
    "### Ridge Regression:\n",
    "    \n",
    "\n",
    "    \n",
    "Here, if lambda is zero then you can imagine we get back OLS. However, if lambda is very large then it will add too much weight and it will lead to under-fitting. Having said that it’s important how lambda is chosen. This technique works very well to avoid over-fitting issue.\n",
    "\n",
    "### Lasso Regression:\n",
    "    \n",
    "(Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude” of coefficient as penalty term to the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use Normal and Gradient Descent :\n",
    "    \n",
    "When we have more features then we need to use Gradient descent over millions and when we have small data set then we can use normal equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{array}{l}{J(\\theta)=\\frac{1}{2 m}\\left[\\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}+\\lambda \\sum_{j=1}^{n} \\theta_{j}^{2}\\right]} \\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "Following is the normal equation,\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta=\\left(X^{T} X\\right)^{-1} X^{T} y\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
